I"<p><a href="http://rolandtanglao.com/2020/07/29/p1-blogthis-checkvist-list-links-to-blog/">Discovered</a>: Jun 17, 2024 18:28  <a href="https://www.linkedin.com/feed/?highlightedUpdateUrn=urn%3Ali%3Aactivity%3A7205335701167501312&amp;trk=eml-email_notification_digest_01-hero_notification_cta-0-SHARED_BY_YOUR_NETWORK"> How to run a 100% local, fully private LLM with llama.cpp ðŸ”¥
 Â¦ LinkedIn</a> â€“&gt; <strong>QUOTE</strong>: <code class="language-plaintext highlighter-rouge">How to run a 100% local, fully private LLM with llama.cpp ðŸ”¥ ... 2 lines of code, OpenAI compatible! ... Step 1: brew install llama.cpp ...Step 2: llama-server --hf-repo microsoft/Phi-3-mini-4k-instruct-gguf --hf-file Phi-3-mini-4k-instruct-q4.gguf ... Step 3: curl 8080/v1/chat/completions </code> Also ` Step 1: Run the above instructions on your local computer Step 2: npm i holesail -g Step 3: holesail â€“live 8080 â€“host localhost<code class="language-plaintext highlighter-rouge"> and </code>python -m llama_cpp.server â€“model XXX is way simpler`</p>
:ET