---
layout: post
title: " How to run a 100% local, fully private LLM with llama.cpp ðŸ”¥
 | LinkedIn"
---
[Discovered](http://rolandtanglao.com/2020/07/29/p1-blogthis-checkvist-list-links-to-blog/): Jun 17, 2024 18:28  [ How to run a 100% local, fully private LLM with llama.cpp ðŸ”¥
 Â¦ LinkedIn](https://www.linkedin.com/feed/?highlightedUpdateUrn=urn%3Ali%3Aactivity%3A7205335701167501312&trk=eml-email_notification_digest_01-hero_notification_cta-0-SHARED_BY_YOUR_NETWORK) --> **QUOTE**: `How to run a 100% local, fully private LLM with llama.cpp ðŸ”¥ ... 2 lines of code, OpenAI compatible! ... Step 1: brew install llama.cpp ...Step 2: llama-server --hf-repo microsoft/Phi-3-mini-4k-instruct-gguf --hf-file Phi-3-mini-4k-instruct-q4.gguf ... Step 3: curl 8080/v1/chat/completions ` Also ` Step 1: Run the above instructions on your local computer Step 2: npm i holesail -g Step 3: holesail --live 8080 --host localhost` and `python -m llama_cpp.server --model XXX is way simpler`
